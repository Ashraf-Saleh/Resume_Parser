{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Parser Documentation\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides a Python-based solution to parse a resume PDF document and extract key information, including:\n",
    "\n",
    "1. **Full Name**\n",
    "2. **Contact Information** (e.g., email, phone, LinkedIn)\n",
    "3. **Summary or Objective Statement**\n",
    "4. **Skills** (as a list)\n",
    "5. **Work Experience** (including company, job title, dates, and responsibilities)\n",
    "6. **Education** (degree, institution, dates, and additional information)\n",
    "7. **Certifications** (if present)\n",
    "8. **Projects** (if present)\n",
    "\n",
    "The parser is designed to handle diverse resume formats by utilizing PDF text extraction and regular expressions to identify and gather relevant information.\n",
    "\n",
    "## Libraries Used\n",
    "\n",
    "- `pdfminer`: For extracting text from PDF files.\n",
    "- `re`: For regular expression matching to locate and extract specific resume sections.\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "- Each section (e.g., Skills, Work Experience) is identified by common headers like \"Skills\" or \"Work Experience.\"\n",
    "- Contact information, such as **email** and **phone number**, typically appears near the beginning of the resume.\n",
    "- Not all resumes contain all requested sections, so the parser handles missing information gracefully, returning empty fields for missing sections.\n",
    "\n",
    "## Challenges\n",
    "\n",
    "- **Format Variations**: Resume layouts vary widely, so this parser uses flexible regex patterns and generic keywords to identify each section.\n",
    "- **Two-Column Layouts**: PDFs sometimes store text in a non-linear order, especially in multi-column layouts, making extraction challenging. This parser attempts to handle these cases but may require further adjustment for highly complex layouts.\n",
    "\n",
    "## Instructions for Use\n",
    "\n",
    "1. Run each code cell in sequence.\n",
    "2. Provide the path to a resume PDF file when prompted.\n",
    "3. The parser will output a structured dictionary with the parsed resume information.\n",
    "\n",
    "## Expected Output Format\n",
    "\n",
    "The output is a Python dictionary with keys representing each information type, such as:\n",
    "```python\n",
    "{\n",
    "    \"Full Name\": \"John Doe\",\n",
    "    \"Contact Information\": {\n",
    "        \"Email\": \"john.doe@example.com\",\n",
    "        \"Phone\": \"123-456-7890\",\n",
    "        \"LinkedIn\": \"linkedin.com/in/johndoe\"\n",
    "    },\n",
    "    \"Summary\": \"Experienced data professional...\",\n",
    "    \"Skills\": [\"Python\", \"SQL\", \"Machine Learning\", \"Data Analysis\"],\n",
    "    \"Work Experience\": [\n",
    "        {\n",
    "            \"Company\": \"Example Corp\",\n",
    "            \"Job Title\": \"Data Scientist\",\n",
    "            \"Dates\": \"Jan 2018 - Dec 2020\",\n",
    "            \"Responsibilities\": \"Developed machine learning models...\"\n",
    "        }\n",
    "    ],\n",
    "    \"Education\": [\n",
    "        {\n",
    "            \"Degree\": \"BSc. Computer Science\",\n",
    "            \"Institution\": \"University of XYZ\",\n",
    "            \"Dates\": \"2014 - 2018\"\n",
    "        }\n",
    "    ],\n",
    "    \"Certifications\": [\"Certified Data Scientist\"],\n",
    "    \"Projects\": [\"Project A: Developed a recommendation system...\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gilbert Adjei \n",
      "Gilbert is an experienced data professional with 6+ years of experience in data engineering, analytics & software engineering. Career assignments have\n",
      "ranged from building data science solutions for startups to leading project teams with eﬀective communication & teamwork. With this background, he\n",
      "is adept at picking up new skills quickly to deliver robust solutions to the most demanding of businesses. \n",
      "gilbertadjei800@gmail.com \n",
      "linkedin.com/in/gilbert-adjei-900ba2110 \n",
      "github.com/GilbertAbakahAdjei \n",
      "medium.com/@gilbertadjei800 \n",
      "WORK EXPERIENCE \n",
      "Data Engineering Consultant \n",
      "Entelligent \n",
      "04/2022 - Present\n",
      ", \n",
      " \n",
      "Colorado, US \n",
      "Mentored data engineering apprentices & new hires to build new\n",
      "climate risk measures while ensuring data quality and architecture\n",
      "initiatives are met. \n",
      "Led the build out of A.I data product and a platform that supports\n",
      "analytics, product development, and product delivery. \n",
      "Lead Data & Analytics Engineer \n",
      "Float (YC W'20) \n",
      "09/2021 - 04/2022\n",
      ", \n",
      " \n",
      "San Francisco, US · Remote \n",
      "Led Data Science & Engineering team to build data products +\n",
      "engage in strategic partnerships to solve cashﬂow and working\n",
      "capital problems for Africa's SMEs \n",
      "Senior Data Engineer \n",
      "SuperFluid Labs, Ltd \n",
      "04/2020 - 09/2021\n",
      ", \n",
      " \n",
      "Remote \n",
      "Built a data product that generated credit scores and credit limits\n",
      "to be assigned to a Telco's customer base of about 25 million\n",
      "customers. \n",
      "Led data engineering, data governance, data quality and\n",
      "architecture initiatives. \n",
      "Data Science & Software Engineering Consultant\n",
      "SuperFluid Labs, Ltd \n",
      "03/2019 - 04/2020\n",
      ", \n",
      " \n",
      "Accra, Ghana \n",
      "Slashed non repayment of loans by 40% by building a data product\n",
      "that created an optimal loan portfolio that maximizes returns on\n",
      "no-collateral, short-term, cash-based loans targeted at agro-dealers\n",
      "and farmers. \n",
      "Developed a dynamic customer analytics and scoring platform that\n",
      "allows various enterprise stakeholders to drive business growth\n",
      "through actionable customer insights and scoring from customer,\n",
      "business and product data. \n",
      "Products Manager - Data Science \n",
      "SGS GHANA \n",
      "08/2017 - 08/2018\n",
      ", \n",
      " \n",
      "Reduced maternal mortality rate by 35% by leading team of ﬁve(5)\n",
      "engineers to build SGS Collect, a data product, which is used as a\n",
      "recommendation tool to pregnant women. \n",
      "EDUCATION \n",
      "BSc. Mathematics & Computer Science \n",
      "University of Ghana, Legon. \n",
      "09/2013 - 07/2017\n",
      ", \n",
      " \n",
      "SKILLS \n",
      "Python \n",
      "SQL & MongoDB \n",
      "DBT \n",
      "Kubernetes \n",
      "Spark \n",
      "Docker \n",
      "GIT \n",
      "Snowﬂake \n",
      "Analytics \n",
      "Metabase \n",
      "Airbyte \n",
      "Airﬂow \n",
      "AWS \n",
      "Scala \n",
      "A.I \n",
      "Statistics \n",
      "Power BI \n",
      "FastAPI \n",
      "PUBLICATIONS & CONFERENCES \n",
      "Transfer Learning with PyTorch\n",
      " \n",
      " \n",
      "Research published in Heartbeat, HackerNews & Reddit \n",
      "Tuning machine learning hyperparameters\n",
      " \n",
      " \n",
      "Research published in Heartbeat, HackerNews & Reddit \n",
      "Understanding Naive Bayes & its applications in text\n",
      "classiﬁcation\n",
      " \n",
      " \n",
      "Research published in Heartbeat, HackerNews & Reddit \n",
      "Deploying and Hosting a Machine Learning Model Using Flask,\n",
      "Heroku and Gunicorn\n",
      " \n",
      " \n",
      "Research published in Heartbeat, HackerNews & Reddit \n",
      "Speaker, Ghana Data Science Summit 2019. \n",
      "Mathematics for Machine Learning \n",
      "AWARDS & CERTIFICATIONS \n",
      "Winner – AfriHack Data Science Challenge, 2019. \n",
      "Higher Calculus & Functions\n",
      " (10/2018 - Present)\n",
      " \n",
      " \n",
      "Duke University Data Science Math Skills\n",
      " (11/2018 - Present)\n",
      " \n",
      "University of Michigan Intro to Data Science\n",
      " (09/2018 - Present)\n",
      " \n",
      " \n",
      "IBM Machine Learning with Python\n",
      " (10/2018 - Present)\n",
      " \n",
      " \n",
      "IBM Data Science Professional Certiﬁcate\n",
      " (10/2018 - Present)\n",
      " \n",
      " \n",
      "Agile Scrum Foundation\n",
      " (08/2017 - Present)\n",
      " \n",
      " \n",
      "INTERESTS \n",
      "Chess \n",
      "A.I \n",
      "Football \n",
      "Achievements/Tasks \n",
      "Achievements/Tasks \n",
      "Achievements/Tasks \n",
      "Achievements/Tasks \n",
      "Achievements/Tasks \n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text = page.extract_text()\n",
    "            # print(f\"Page {page_num + 1}:\\n{text}\\n\")\n",
    "    return text\n",
    "\n",
    "# Example usage:\n",
    "pdf_file = \"data/Sample Resume for Assessment.pdf\"\n",
    "result = extract_text_from_pdf(pdf_file)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_name(text):\n",
    "    \"\"\"Extracts the name, assuming it is the first line with capitalized words.\"\"\"\n",
    "    # Split text into lines\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    # Regex pattern for a typical name format (two capitalized words)\n",
    "    name_pattern = re.compile(r\"^[A-Z][a-zA-Z]*\\s+[A-Z][a-zA-Z]*$\")\n",
    "\n",
    "    # Look for the first line that matches the name pattern\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if name_pattern.match(line):\n",
    "            return line\n",
    "\n",
    "    return \"Name not found\"\n",
    "name = extract_name(result)\n",
    "print(\"Extracted Name:\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_accounts_from_resume(text):\n",
    "    contact_number = None\n",
    "\n",
    "    # Regex pattern to capture URLs and email addresses\n",
    "    pattern = r'(https?://)?(www\\.)?([a-zA-Z0-9-]+)\\.[a-zA-Z]+(/\\S*)?|([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})'\n",
    "\n",
    "    # Dictionary to store results\n",
    "    accounts = {}\n",
    "\n",
    "    # Find all matches\n",
    "    matches = re.findall(pattern, text)\n",
    "\n",
    "    # Process each match to build the accounts dictionary\n",
    "    for match in matches:\n",
    "        full_url = ''.join(match[:4])  # URL components are in the first four groups\n",
    "        email = match[4]  # Email is in the fifth group\n",
    "\n",
    "        if full_url:  # If a URL is found\n",
    "            domain_name = match[2]  # Third capture group is the domain (e.g., \"github\", \"linkedin\")\n",
    "            if domain_name and len(domain_name) > 1:\n",
    "                if domain_name in accounts:\n",
    "                    accounts[domain_name].append(full_url)\n",
    "                else:\n",
    "                    accounts[domain_name] = [full_url]\n",
    "\n",
    "        if email:  # If an email is found\n",
    "            email_domain = email.split('@')[1].split('.')[0]  # Get domain before the first dot in domain part\n",
    "            if email_domain in accounts:\n",
    "                accounts[email_domain].append(email)\n",
    "            else:\n",
    "                accounts[email_domain] = [email]\n",
    "\n",
    "    return accounts\n",
    "\n",
    "\n",
    "accounts = extract_accounts_from_resume(result)\n",
    "print(\"Textracted Accounts: \", accounts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resume_parser_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
