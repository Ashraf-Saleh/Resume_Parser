{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Parser Documentation\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides a comprehensive Python-based solution to parse resume PDFs and extract key information, including:\n",
    "\n",
    "1. **Full Name**\n",
    "2. **Contact Information** (e.g., email, phone, LinkedIn)\n",
    "3. **Summary or Objective Statement**\n",
    "4. **Skills** (as a list)\n",
    "5. **Work Experience** (including company, job title, dates, and responsibilities)\n",
    "6. **Education** (degree, institution, dates, and additional information)\n",
    "7. **Certifications and Awards** (if present)\n",
    "8. **Projects** (if present)\n",
    "\n",
    "The parser is designed to handle diverse resume formats, leveraging PDF text extraction, regular expressions, and natural language processing to identify and extract each section.\n",
    "\n",
    "## Libraries Used\n",
    "\n",
    "- `PyPDF2`: For opening and reading text from PDF files.\n",
    "- `PyMuPDF`: For extracting text with font information, enabling more accurate section identification.\n",
    "- `re`: For regular expressions to locate and match sections, dates, and other structured information.\n",
    "- `spacy`: For advanced natural language processing to detect names and segment text.\n",
    "\n",
    "## Notebook Structure\n",
    "\n",
    "Each cell in the notebook is organized to focus on one key component of resume parsing:\n",
    "\n",
    "1. **Import Libraries for PDF Extraction**: Imports all necessary libraries.\n",
    "2. **Function to Extract Text from PDF (using PyPDF2)**: Reads PDF content to extract text.\n",
    "3. **Function to Extract Sections with Font Information (using PyMuPDF)**: Extracts text with font details to aid in identifying sections.\n",
    "4. **Function to Extract Name from Text**: Uses `spacy` to detect a full name.\n",
    "5. **Extracting Summary/Objective from Resume Text**: Identifies and extracts the summary or objective section.\n",
    "6. **Function to Extract Accounts (URLs and Emails) from Resume**: Extracts contact details like email and LinkedIn profile links.\n",
    "7. **Function to Extract Skills from Resume Sections**: Extracts skills listed within a specific section.\n",
    "8. **Function to Extract Education Details from Resume**: Identifies and extracts educational information, including degrees and institutions.\n",
    "9. **Function to Extract Certifications and Awards**: Searches for certification or award-related sections and gathers details.\n",
    "10. **Function to Extract Projects**: Identifies and extracts project information.\n",
    "11. **Extract Work Experience from Resume**: Extracts details such as job title, company, dates, and responsibilities.\n",
    "\n",
    "## Challenges\n",
    "\n",
    "- **Format Variations**: Resume layouts vary widely, so this parser uses flexible regex patterns and generic keywords to identify each section across different styles and formats.\n",
    "- **Two-Column Layouts**: PDFs sometimes store text in a non-linear order, especially in multi-column layouts, making extraction challenging. The parser attempts to handle this by processing text with structure-aware extraction methods, though highly complex layouts may still require further refinement.\n",
    "\n",
    "## Instructions for Use\n",
    "\n",
    "1. Run each code cell in sequence.\n",
    "2. Provide the path to a resume PDF file when prompted.\n",
    "3. The parser will output a structured dictionary containing the parsed resume information.\n",
    "\n",
    "## Expected Output Format\n",
    "\n",
    "The parsed resume information is output as a structured dictionary with keys representing each information type:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"Full Name\": \"John Doe\",\n",
    "    \"Contact Information\": {\n",
    "        \"Email\": \"john.doe@example.com\",\n",
    "        \"Phone\": \"123-456-7890\",\n",
    "        \"LinkedIn\": \"linkedin.com/in/johndoe\"\n",
    "    },\n",
    "    \"Summary\": \"Experienced data professional...\",\n",
    "    \"Skills\": [\"Python\", \"SQL\", \"Machine Learning\", \"Data Analysis\"],\n",
    "    \"Work Experience\": [\n",
    "        {\n",
    "            \"Company\": \"Example Corp\",\n",
    "            \"Job Title\": \"Data Scientist\",\n",
    "            \"Dates\": \"Jan 2018 - Dec 2020\",\n",
    "            \"Responsibilities\": \"Developed machine learning models...\"\n",
    "        }\n",
    "    ],\n",
    "    \"Education\": [\n",
    "        {\n",
    "            \"Degree\": \"BSc. Computer Science\",\n",
    "            \"Institution\": \"University of XYZ\",\n",
    "            \"Dates\": \"2014 - 2018\"\n",
    "        }\n",
    "    ],\n",
    "    \"Certifications\": [\"Certified Data Scientist\"],\n",
    "    \"Projects\": [\"Project A: Developed a recommendation system...\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries for PDF Extraction\n",
    "\n",
    "This cell imports the necessary libraries for extracting and analyzing text from PDF documents. We use `PyPDF2` for basic PDF text extraction, `fitz` (PyMuPDF) for advanced text extraction with font information, `re` for regular expressions, and `spacy` for natural language processing. Additionally, we load the English NLP model from spaCy (`en_core_web_sm`) for named entity recognition and language processing tasks.\n",
    "\n",
    "**Input:**\n",
    "- None\n",
    "\n",
    "**Output:**\n",
    "- None (Library imports for setting up extraction and NLP tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashraf/anaconda3/envs/resume_parser_mac_env/lib/python3.11/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "# Load the English NLP model from spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Extract Text from PDF (using PyPDF2)\n",
    "This function reads a PDF file and extracts plain text from it using the PyPDF2 library.\n",
    "\n",
    "**Input:**\n",
    "- `pdf_path` (str): The file path of the PDF.\n",
    "\n",
    "**Output:**\n",
    "- `text` (str): The extracted text from the PDF file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gilbert Adjei \n",
      "Gilbert is an experienced data professional with 6+ years of experience in data engineering, analytics & software engineering. Career assignments have\n",
      "ranged from building data science solutions for startups to leading project teams with eﬀective communication & teamwork. With this background, he\n",
      "is adept at picking up new skills quickly to deliver robust solutions to the most demanding of businesses. \n",
      "gilbertadjei800@gmail.com \n",
      "linkedin.com/in/gilbert-adjei-900ba2110 \n",
      "github.com/GilbertAbakahAdjei \n",
      "medium.com/@gilbertadjei800 \n",
      "WORK EXPERIENCE \n",
      "Data Engineering Consultant \n",
      "Entelligent \n",
      "04/2022 - Present\n",
      ", \n",
      " \n",
      "Colorado, US \n",
      "Mentored data engineering apprentices & new hires to build new\n",
      "climate risk measures while ensuring data quality and architecture\n",
      "initiatives are met. \n",
      "Led the build out of A.I data product and a platform that supports\n",
      "analytics, product development, and product delivery. \n",
      "Lead Data & Analytics Engineer \n",
      "Float (YC W'20) \n",
      "09/2021 - 04/2022\n",
      ", \n",
      " \n",
      "San Francisco, US · Remote \n",
      "Led Data Science & Engineering team to build data products +\n",
      "engage in strategic partnerships to solve cashﬂow and working\n",
      "capital problems for Africa's SMEs \n",
      "Senior Data Engineer \n",
      "SuperFluid Labs, Ltd \n",
      "04/2020 - 09/2021\n",
      ", \n",
      " \n",
      "Remote \n",
      "Built a data product that generated credit scores and credit limits\n",
      "to be assigned to a Telco's customer base of about 25 million\n",
      "customers. \n",
      "Led data engineering, data governance, data quality and\n",
      "architecture initiatives. \n",
      "Data Science & Software Engineering Consultant\n",
      "SuperFluid Labs, Ltd \n",
      "03/2019 - 04/2020\n",
      ", \n",
      " \n",
      "Accra, Ghana \n",
      "Slashed non repayment of loans by 40% by building a data product\n",
      "that created an optimal loan portfolio that maximizes returns on\n",
      "no-collateral, short-term, cash-based loans targeted at agro-dealers\n",
      "and farmers. \n",
      "Developed a dynamic customer analytics and scoring platform that\n",
      "allows various enterprise stakeholders to drive business growth\n",
      "through actionable customer insights and scoring from customer,\n",
      "business and product data. \n",
      "Products Manager - Data Science \n",
      "SGS GHANA \n",
      "08/2017 - 08/2018\n",
      ", \n",
      " \n",
      "Reduced maternal mortality rate by 35% by leading team of ﬁve(5)\n",
      "engineers to build SGS Collect, a data product, which is used as a\n",
      "recommendation tool to pregnant women. \n",
      "EDUCATION \n",
      "BSc. Mathematics & Computer Science \n",
      "University of Ghana, Legon. \n",
      "09/2013 - 07/2017\n",
      ", \n",
      " \n",
      "SKILLS \n",
      "Python \n",
      "SQL & MongoDB \n",
      "DBT \n",
      "Kubernetes \n",
      "Spark \n",
      "Docker \n",
      "GIT \n",
      "Snowﬂake \n",
      "Analytics \n",
      "Metabase \n",
      "Airbyte \n",
      "Airﬂow \n",
      "AWS \n",
      "Scala \n",
      "A.I \n",
      "Statistics \n",
      "Power BI \n",
      "FastAPI \n",
      "PUBLICATIONS & CONFERENCES \n",
      "Transfer Learning with PyTorch\n",
      " \n",
      " \n",
      "Research published in Heartbeat, HackerNews & Reddit \n",
      "Tuning machine learning hyperparameters\n",
      " \n",
      " \n",
      "Research published in Heartbeat, HackerNews & Reddit \n",
      "Understanding Naive Bayes & its applications in text\n",
      "classiﬁcation\n",
      " \n",
      " \n",
      "Research published in Heartbeat, HackerNews & Reddit \n",
      "Deploying and Hosting a Machine Learning Model Using Flask,\n",
      "Heroku and Gunicorn\n",
      " \n",
      " \n",
      "Research published in Heartbeat, HackerNews & Reddit \n",
      "Speaker, Ghana Data Science Summit 2019. \n",
      "Mathematics for Machine Learning \n",
      "AWARDS & CERTIFICATIONS \n",
      "Winner – AfriHack Data Science Challenge, 2019. \n",
      "Higher Calculus & Functions\n",
      " (10/2018 - Present)\n",
      " \n",
      " \n",
      "Duke University Data Science Math Skills\n",
      " (11/2018 - Present)\n",
      " \n",
      "University of Michigan Intro to Data Science\n",
      " (09/2018 - Present)\n",
      " \n",
      " \n",
      "IBM Machine Learning with Python\n",
      " (10/2018 - Present)\n",
      " \n",
      " \n",
      "IBM Data Science Professional Certiﬁcate\n",
      " (10/2018 - Present)\n",
      " \n",
      " \n",
      "Agile Scrum Foundation\n",
      " (08/2017 - Present)\n",
      " \n",
      " \n",
      "INTERESTS \n",
      "Chess \n",
      "A.I \n",
      "Football \n",
      "Achievements/Tasks \n",
      "Achievements/Tasks \n",
      "Achievements/Tasks \n",
      "Achievements/Tasks \n",
      "Achievements/Tasks \n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()  # Accumulate text from each page\n",
    "    return text\n",
    "\n",
    "# Example usage:\n",
    "pdf_file = \"data/Sample Resume for Assessment.pdf\"\n",
    "result = extract_text_from_pdf(pdf_file)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Extract Sections with Font Information (using PyMuPDF)\n",
    "This function extracts the text and associated font details (like size and name) from each section in the PDF using `PyMuPDF`. It also distinguishes between titles and content.\n",
    "\n",
    "**Input:**\n",
    "- `pdf_path` (str): The file path of the PDF.\n",
    "\n",
    "**Output:**\n",
    "- `sections` (dict): A dictionary where each key is a section title (e.g., \"Work Experience\") and the value is a list of dictionaries containing text and font info.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sections_with_font_info(pdf_path):\n",
    "    document = fitz.open(pdf_path)\n",
    "    sections = {}  # Dictionary to store sections with text and font info\n",
    "    current_title = None\n",
    "    current_content = []\n",
    "\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "\n",
    "        for block in blocks:\n",
    "            if \"lines\" in block:\n",
    "                for line in block[\"lines\"]:\n",
    "                    for span in line[\"spans\"]:\n",
    "                        text = span[\"text\"].strip()\n",
    "                        font_size = span[\"size\"]\n",
    "                        font_name = span[\"font\"]\n",
    "                        \n",
    "                        if re.match(r\"^[A-Z]{4,}\\b.*\", text):  # Title line pattern\n",
    "                            if current_title:\n",
    "                                sections[current_title] = current_content\n",
    "                            current_title = text\n",
    "                            current_content = [{\"text\": text, \"font_size\": font_size, \"font_name\": font_name}]\n",
    "                        else:\n",
    "                            if current_title:\n",
    "                                current_content.append({\"text\": text, \"font_size\": font_size, \"font_name\": font_name})\n",
    "\n",
    "    if current_title:\n",
    "        sections[current_title] = current_content\n",
    "\n",
    "    document.close()\n",
    "    return sections\n",
    "\n",
    "# Example usage\n",
    "extracted_sections = extract_sections_with_font_info(pdf_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Extract Name from Text\n",
    "This function searches the extracted text for a name pattern, which typically consists of two capitalized words.\n",
    "\n",
    "**Input:**\n",
    "- `text` (str): The extracted text from the resume.\n",
    "\n",
    "**Output:**\n",
    "- `name` (str): The extracted name or \"Name not found\" if no valid name is detected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Name: Gilbert Adjei\n"
     ]
    }
   ],
   "source": [
    "def extract_name(text):\n",
    "    lines = text.splitlines()\n",
    "    name_pattern = re.compile(r\"^[A-Z][a-zA-Z]*\\s+[A-Z][a-zA-Z]*$\")\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if name_pattern.match(line):\n",
    "            return line\n",
    "\n",
    "    return \"Name not found\"\n",
    "\n",
    "name = extract_name(result)\n",
    "print(\"Extracted Name:\", name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Summary/Objective from Resume Text\n",
    "This function extracts the summary or objective section from a resume. It looks for section titles and extracts the corresponding content.\n",
    "\n",
    "**Input:**\n",
    "- `text` (str): The extracted text from the resume.\n",
    "\n",
    "**Output:**\n",
    "- `summary_text` (str): The extracted summary or objective.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Summary/Objective:\n",
      "Gilbert is an experienced data professional with 6+ years of experience in data engineering, analytics & software engineering. Career assignments haveranged from building data science solutions for startups to leading project teams with eﬀective communication & teamwork. With this background, he is adept at picking up new skills quickly to deliver robust solutions to the most demanding of businesses.\n"
     ]
    }
   ],
   "source": [
    "summary_title_pattern = (\n",
    "    r\"^(Summary|Professional Summary|Career Summary|Executive Summary|Summary of Qualifications|\"\n",
    "    r\"Profile|Professional Profile|Personal Profile|Career Profile|Personal Summary|Overview|\"\n",
    "    r\"Objective|Career Objective|Professional Objective|Statement|Introduction|About Me)\\s*$\"\n",
    ")\n",
    "\n",
    "uppercase_title_pattern = r\"^[A-Z]{4,}\\b.*\"  # Matches lines that are fully uppercase with more than 4 characters\n",
    "name_pattern = r\"^[A-Za-z\\s]+$\"  # Basic pattern to match a name (adjust as needed)\n",
    "\n",
    "lines = result.splitlines()\n",
    "summary_text = \"\"\n",
    "name_found = False\n",
    "is_capturing = False\n",
    "\n",
    "for line in lines:\n",
    "    if re.match(summary_title_pattern, line, re.IGNORECASE):\n",
    "        is_capturing = True\n",
    "        continue\n",
    "    elif is_capturing:\n",
    "        if re.match(uppercase_title_pattern, line):\n",
    "            break\n",
    "        elif line.strip():\n",
    "            summary_text += line.strip() + \" \"\n",
    "            if line.strip().endswith('.'):\n",
    "                break\n",
    "    elif re.match(name_pattern, line) and not name_found:\n",
    "        name_found = True\n",
    "    elif name_found and not is_capturing:\n",
    "        summary_text = line.strip()\n",
    "        is_capturing = True\n",
    "\n",
    "print(\"Extracted Summary/Objective:\")\n",
    "print(summary_text.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Extract Accounts (URLs and Emails) from Resume\n",
    "This function extracts any URLs or email addresses from the resume text. It uses regular expressions to find and categorize them.\n",
    "\n",
    "**Input:**\n",
    "- `text` (str): The extracted text from the resume.\n",
    "\n",
    "**Output:**\n",
    "- `accounts` (dict): A dictionary where keys are domains (e.g., \"github\", \"linkedin\") and values are lists of matching URLs or emails.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accounts:  {'gmail': ['gilbertadjei800@gmail.com'], 'linkedin': ['linkedin/in/gilbert-adjei-900ba2110'], 'github': ['github/GilbertAbakahAdjei'], 'medium': ['medium/@gilbertadjei800']}\n"
     ]
    }
   ],
   "source": [
    "def extract_accounts_from_resume(text):\n",
    "    contact_number = None\n",
    "    pattern = r'(https?://)?(www\\.)?([a-zA-Z0-9-]+)\\.[a-zA-Z]+(/\\S*)?|([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})'\n",
    "    accounts = {}\n",
    "\n",
    "    matches = re.findall(pattern, text)\n",
    "    for match in matches:\n",
    "        full_url = ''.join(match[:4])\n",
    "        email = match[4]\n",
    "\n",
    "        if full_url:\n",
    "            domain_name = match[2]\n",
    "            if domain_name and len(domain_name) > 1:\n",
    "                if domain_name in accounts:\n",
    "                    accounts[domain_name].append(full_url)\n",
    "                else:\n",
    "                    accounts[domain_name] = [full_url]\n",
    "\n",
    "        if email:\n",
    "            email_domain = email.split('@')[1].split('.')[0]\n",
    "            if email_domain in accounts:\n",
    "                accounts[email_domain].append(email)\n",
    "            else:\n",
    "                accounts[email_domain] = [email]\n",
    "\n",
    "    return accounts\n",
    "\n",
    "accounts = extract_accounts_from_resume(result)\n",
    "print(\"accounts: \", accounts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Extract Skills from Resume Sections\n",
    "This function extracts skills-related information from the resume. It identifies sections such as \"Skills\", \"Technical Skills\", etc., and retrieves their content.\n",
    "\n",
    "**Input:**\n",
    "- `extracted_sections` (dict): The dictionary of sections extracted from the resume, with each section having associated content and font size.\n",
    "\n",
    "**Output:**\n",
    "- `skills` (list): A list of skills extracted from the resume.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python\n",
      "SQL & MongoDB\n",
      "DBT\n",
      "Kubernetes\n",
      "Spark\n",
      "Docker\n",
      "GIT\n",
      "Snowﬂake\n",
      "Analytics\n",
      "Metabase\n",
      "Airbyte\n",
      "Airﬂow\n",
      "AWS\n",
      "Scala\n",
      "A.I\n",
      "Statistics\n",
      "Power BI\n",
      "FastAPI\n"
     ]
    }
   ],
   "source": [
    "skill_section_keywords = [\n",
    "    \"Skills\", \"Technical Skills\", \"Core Competencies\", \"Technical Proficiencies\",\n",
    "    \"IT Skills\", \"Programming Skills\", \"Software Skills\", \"Data Skills\",\n",
    "    \"Engineering Skills\", \"Hard Skills\", \"Technical Abilities\", \"Core Skills\",\n",
    "    \"Key Skills\", \"Professional Skills\", \"Skill Set\", \"Competencies\",\n",
    "    \"Expertise\", \"Areas of Expertise\", \"Technical Competencies\",\n",
    "    \"Technical Expertise\", \"Technical Proficiencies\", \"Proficiencies\",\n",
    "    \"Core Proficiencies\", \"Technical Abilities\", \"Technical Acumen\",\n",
    "    \"Relevant Skills\", \"Specialized Skills\", \"Tools and Technologies\"\n",
    "]\n",
    "\n",
    "skills_pattern = re.compile(r\"(\" + \"|\".join(skill_section_keywords) + \")\", re.IGNORECASE)\n",
    "\n",
    "skills = []\n",
    "for title, content in extracted_sections.items():\n",
    "    if skills_pattern.search(title):\n",
    "        skills_items = [item for item in content if item[\"font_size\"] < content[0][\"font_size\"]]\n",
    "        skills.extend([item[\"text\"] for item in skills_items])\n",
    "\n",
    "for skill in skills:\n",
    "    print(skill)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Extract Education Details from Resume\n",
    "This function identifies the \"Education\" section in the resume and extracts relevant information such as degree, institution, and dates.\n",
    "\n",
    "**Input:**\n",
    "- `extracted_sections` (dict): The dictionary of sections extracted from the resume.\n",
    "\n",
    "**Output:**\n",
    "- `education_list` (list): A list of dictionaries containing education details such as degree, institution, and dates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'degree': None, 'institution': 'University of Ghana, Legon.', 'dates': '09/2013 - 07/2017'}\n"
     ]
    }
   ],
   "source": [
    "education_section_titles = [\n",
    "    \"Education\", \"Educational Background\", \"Academic Background\",\n",
    "    \"Academic History\", \"Educational Qualifications\", \"Academic Qualifications\",\n",
    "    \"Academic Experience\", \"Education and Training\", \"Formal Education\",\n",
    "    \"Scholastic Background\", \"Scholarly Background\", \"Education Details\",\n",
    "    \"Academic Record\", \"Academic Credentials\", \"Education Summary\",\n",
    "    \"Educational Achievements\", \"Education History\", \"Professional Education\",\n",
    "    \"Training and Education\"\n",
    "]\n",
    "\n",
    "education_pattern = re.compile(r\"(\" + \"|\".join(education_section_titles) + \")\", re.IGNORECASE)\n",
    "\n",
    "education_list = []\n",
    "for title, content in extracted_sections.items():\n",
    "    if education_pattern.search(title):\n",
    "        education_entries = [\n",
    "            item[\"text\"] for item in content if item[\"font_size\"] < content[0][\"font_size\"]\n",
    "        ]\n",
    "        \n",
    "        degree_pattern = r\"(Bachelor|Master|Doctor|Associate|PhD)\"\n",
    "        institution = None\n",
    "        degree = None\n",
    "        dates = None\n",
    "\n",
    "        for entry in education_entries:\n",
    "            if re.search(degree_pattern, entry):\n",
    "                degree = entry\n",
    "            elif \"University\" in entry or \"College\" in entry:\n",
    "                institution = entry\n",
    "            elif re.search(r\"\\d{4}\", entry):\n",
    "                dates = entry\n",
    "\n",
    "        education_list.append({\"degree\": degree, \"institution\": institution, \"dates\": dates})\n",
    "\n",
    "for edu in education_list:\n",
    "    print(edu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Extract Certifications and Awards\n",
    "This function identifies the \"Certifications\" or \"Awards\" section in the resume, searches for common section titles, and extracts relevant text content.\n",
    "\n",
    "**Function Name:**\n",
    "- `extract_certifications`\n",
    "\n",
    "**Input:**\n",
    "- `sections` (dict): A dictionary where keys are section titles and values are lists of text elements, each with attributes like `text`, `font_size`, and `font_name`.\n",
    "\n",
    "**Output:**\n",
    "- `certifications` (list): A list of certification or award entries extracted from the resume.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Certifications and Awards:\n",
      "- Winner – AfriHack Data Science Challenge, 2019.\n",
      "- Higher Calculus & Functions (10/2018 - Present)\n",
      "- \n",
      "- \n",
      "- Duke University Data Science Math Skills (11/2018 - Present)\n",
      "- \n",
      "- University of Michigan Intro to Data Science\n",
      "- (09/2018 - Present)\n",
      "- \n",
      "- \n",
      "- IBM Machine Learning with Python (10/2018 - Present)\n",
      "- \n",
      "- \n",
      "- IBM Data Science Professional Certiﬁcate (10/2018 - Present)\n",
      "- \n",
      "- \n",
      "- Agile Scrum Foundation (08/2017 - Present)\n",
      "- \n",
      "- \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_certifications(sections):\n",
    "    # List of possible names for \"Certification\" or \"Awards\" sections\n",
    "    certification_section_titles = [\n",
    "        \"Certification\", \"Certifications\", \"Certification and Licenses\", \n",
    "        \"Licenses and Certifications\", \"Licenses\", \"Professional Certifications\",\n",
    "        \"Professional Licenses\", \"Credentials\", \"Awards\", \"Honors\", \n",
    "        \"Achievements\", \"Recognitions\", \"Accolades\"\n",
    "    ]\n",
    "\n",
    "    # Compile the regex pattern to match any of these titles\n",
    "    certification_pattern = re.compile(r\"(\" + \"|\".join(certification_section_titles) + \")\", re.IGNORECASE)\n",
    "\n",
    "    # Initialize a list to store all certification entries as dictionaries\n",
    "    certifications = []\n",
    "\n",
    "    # Loop through sections to check for certifications and awards\n",
    "    for title, content in sections.items():\n",
    "        if certification_pattern.search(title):\n",
    "            certifications += [\n",
    "                item[\"text\"] for item in content\n",
    "                if item[\"font_size\"] < content[0][\"font_size\"]  # Exclude title by font size\n",
    "            ]\n",
    "\n",
    "    return certifications\n",
    "\n",
    "# Example usage\n",
    "certifications = extract_certifications(extracted_sections)\n",
    "print(\"Extracted Certifications and Awards:\")\n",
    "for certification in certifications:\n",
    "    print(f\"- {certification}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Extract Projects\n",
    "This function identifies the \"Projects\" section in the resume by searching for common project-related section titles, then extracts relevant text content.\n",
    "\n",
    "**Function Name:**\n",
    "- `extract_projects`\n",
    "\n",
    "**Input:**\n",
    "- `sections` (dict): A dictionary where keys are section titles and values are lists of text elements, each with attributes like `text`, `font_size`, and `font_name`.\n",
    "\n",
    "**Output:**\n",
    "- `projects` (list): A list of project entries extracted from the resume.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Projects:\n"
     ]
    }
   ],
   "source": [
    "def extract_projects(sections):\n",
    "    # List of possible names for the \"Projects\" section\n",
    "    project_section_titles = [\n",
    "        \"Projects\", \"Project Experience\", \"Professional Projects\", \"Personal Projects\",\n",
    "        \"Key Projects\", \"Relevant Projects\", \"Work Projects\", \"Technical Projects\",\n",
    "        \"Major Projects\", \"Significant Projects\", \"Project Highlights\", \"Selected Projects\"\n",
    "    ]\n",
    "\n",
    "    # Compile the regex pattern to match any of these titles\n",
    "    project_pattern = re.compile(r\"(\" + \"|\".join(project_section_titles) + \")\", re.IGNORECASE)\n",
    "\n",
    "    # Initialize a list to store all project entries as dictionaries\n",
    "    projects = []\n",
    "\n",
    "    # Loop through sections to check for projects\n",
    "    for title, content in sections.items():\n",
    "        if project_pattern.search(title):\n",
    "            projects += [\n",
    "                item[\"text\"] for item in content\n",
    "                if item[\"font_size\"] < content[0][\"font_size\"]  # Exclude title by font size\n",
    "            ]\n",
    "\n",
    "    return projects\n",
    "\n",
    "# Example usage\n",
    "projects = extract_projects(extracted_sections)\n",
    "print(\"Extracted Projects:\")\n",
    "for project in projects:\n",
    "    print(f\"- {project}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Work Experience from Resume\n",
    "This function identifies and extracts \"Work Experience\" sections in the resume. It uses regex patterns to match section titles and extracts job titles, company names, dates, and responsibilities.\n",
    "\n",
    "**Input:**\n",
    "- `sections` (dict): A dictionary of sections extracted from the resume.\n",
    "\n",
    "**Output:**\n",
    "- `work_experience` (list): A list of dictionaries with each dictionary containing job title, company, dates, and responsibilities for a work experience entry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'Data Engineering Consultant', 'company': 'Entelligent', 'dates': '04/2022 - Present', 'responsibilities': ['Mentored data engineering apprentices & new hires to build new', 'climate risk measures while ensuring data quality and architecture', 'Led the build out of A.I data product and a platform that supports', 'analytics, product development, and product delivery.']}, {'title': 'Lead Data & Analytics Engineer', 'company': \"Float (YC W'20)\", 'dates': '09/2021 - 04/2022', 'responsibilities': ['San Francisco, US · Remote', 'Led Data Science & Engineering team to build data products +', 'engage in strategic partnerships to solve cashﬂow and working', \"capital problems for Africa's SMEs\"]}, {'title': 'Senior Data Engineer', 'company': 'SuperFluid Labs, Ltd', 'dates': '04/2020 - 09/2021', 'responsibilities': ['Built a data product that generated credit scores and credit limits', \"to be assigned to a Telco's customer base of about 25 million\", 'Led data engineering, data governance, data quality and']}, {'title': 'Data Science & Software Engineering Consultant', 'company': 'SuperFluid Labs, Ltd', 'dates': '03/2019 - 04/2020', 'responsibilities': ['Slashed non repayment of loans by 40% by building a data product', 'that created an optimal loan portfolio that maximizes returns on', 'no-collateral, short-term, cash-based loans targeted at agro-dealers', 'Developed a dynamic customer analytics and scoring platform that', 'allows various enterprise stakeholders to drive business growth', 'through actionable customer insights and scoring from customer,', 'business and product data.']}, {'title': 'Products Manager - Data Science', 'company': 'SGS GHANA', 'dates': '08/2017 - 08/2018', 'responsibilities': ['Reduced maternal mortality rate by 35% by leading team of ﬁve(5)', 'engineers to build SGS Collect, a data product, which is used as a', 'recommendation tool to pregnant women.']}]\n"
     ]
    }
   ],
   "source": [
    "# List of possible names for the \"Work Experience\" section\n",
    "work_experience_titles = [\n",
    "    \"Work Experience\", \"Professional Experience\", \"Employment History\", \"Employment Experience\",\n",
    "    \"Job History\", \"Work History\", \"Career History\", \"Professional Background\", \n",
    "    \"Work Background\", \"Experience\", \"Employment\", \"Relevant Experience\", \"Professional Experience\"\n",
    "]\n",
    "\n",
    "# Compile the regex pattern to match any of these titles\n",
    "work_experience_pattern = re.compile(r\"(\" + \"|\".join(work_experience_titles) + \")\", re.IGNORECASE)\n",
    "\n",
    "def extract_work_experience(sections):\n",
    "    work_experience = []\n",
    "\n",
    "    for title, content in sections.items():\n",
    "        if work_experience_pattern.search(title):\n",
    "            current_entry = {\n",
    "                'title': None,\n",
    "                'company': None,\n",
    "                'dates': None,\n",
    "                'responsibilities': []\n",
    "            }\n",
    "            collecting_responsibilities = False\n",
    "\n",
    "            for i, line in enumerate(content[1:]):  # Skip title\n",
    "                text = line['text'].strip()\n",
    "                font_size = line['font_size']\n",
    "                font_name = line['font_name']\n",
    "\n",
    "                # Determine if the font is bold\n",
    "                is_bold = \"Bold\" in font_name\n",
    "\n",
    "                # Detect job title (Bold and biggest font size)\n",
    "                if is_bold and font_size >= 10:\n",
    "                    # If a job title is already found, save the previous entry and start a new one\n",
    "                    if current_entry['title']:\n",
    "                        work_experience.append(current_entry)\n",
    "                        current_entry = {'title': None, 'company': None, 'dates': None, 'responsibilities': []}\n",
    "                    \n",
    "                    current_entry['title'] = text\n",
    "                    collecting_responsibilities = False\n",
    "\n",
    "                # Detect company name (same size as title but not bold)\n",
    "                elif font_size >= 10 and not is_bold and current_entry['title'] and not current_entry['company']:\n",
    "                    current_entry['company'] = text\n",
    "                    collecting_responsibilities = False\n",
    "\n",
    "                # Detect dates (smaller font size, look for dates or \"Present\")\n",
    "                elif re.search(r'\\b(\\d{2}/\\d{4}|\\d{4})\\b', text) or 'Present' in text:\n",
    "                    current_entry['dates'] = text\n",
    "                    collecting_responsibilities = True  # Start collecting responsibilities after date\n",
    "\n",
    "                # Collect responsibilities (smaller font size and at least 3 words)\n",
    "                elif collecting_responsibilities and font_size < 10:\n",
    "                    if len(text.split()) > 3:  # Check for more than three words\n",
    "                        current_entry['responsibilities'].append(text)\n",
    "\n",
    "            # Append the last work experience entry\n",
    "            if current_entry['title']:\n",
    "                work_experience.append(current_entry)\n",
    "            break  # Break after finding the first matching section\n",
    "\n",
    "    return work_experience\n",
    "\n",
    "# Assuming `extracted_sections` is the dictionary output from the PDF processing\n",
    "\n",
    "# Example usage\n",
    "work_experience = extract_work_experience(extracted_sections)\n",
    "print(work_experience)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resume_parser_mac_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
